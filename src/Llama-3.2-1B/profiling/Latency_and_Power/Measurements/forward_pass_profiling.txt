Layer                                         Output Shape                   GPU Latency (ms)     CPU Latency (ms)     GPU Power (mW)       CPU Power (mW)      
===========================================================================================================================================================
model.embed_tokens                            torch.Size([1, 6, 2048])       0.215008             0.348568             1244.160000          2488.320000         
model.rotary_emb                              torch.Size([1, 6, 64])         0.845312             0.974655             933.120000           2488.320000         
model.layers.0.input_layernorm                torch.Size([1, 6, 2048])       0.450816             0.606537             933.120000           2642.752000         
model.layers.0.self_attn.q_proj               torch.Size([1, 6, 2048])       0.230592             0.341177             933.504000           2954.880000         
model.layers.0.self_attn.k_proj               torch.Size([1, 6, 512])        0.176416             0.300884             933.120000           2488.320000         
model.layers.0.self_attn.v_proj               torch.Size([1, 6, 512])        0.169728             0.276089             933.120000           2487.296000         
model.layers.0.self_attn.o_proj               torch.Size([1, 6, 2048])       0.196000             0.332594             933.120000           3110.400000         
model.layers.0.post_attention_layernorm       torch.Size([1, 6, 2048])       0.445504             0.640869             933.120000           2331.840000         
model.layers.0.mlp.gate_proj                  torch.Size([1, 6, 8192])       0.430752             0.627756             1088.640000          2954.880000         
model.layers.0.mlp.act_fn                     torch.Size([1, 6, 8192])       0.118816             0.232458             933.120000           2642.752000         
model.layers.0.mlp.up_proj                    torch.Size([1, 6, 8192])       0.422880             0.569344             1089.088000          2488.320000         
model.layers.0.mlp.down_proj                  torch.Size([1, 6, 2048])       0.438688             0.598907             1088.640000          2643.840000         
model.layers.1.input_layernorm                torch.Size([1, 6, 2048])       0.450016             0.567675             933.120000           2488.320000         
model.layers.1.self_attn.q_proj               torch.Size([1, 6, 2048])       0.223712             0.322342             933.120000           2799.360000         
model.layers.1.self_attn.k_proj               torch.Size([1, 6, 512])        0.242336             0.349760             933.120000           2488.320000         
model.layers.1.self_attn.v_proj               torch.Size([1, 6, 512])        0.168128             0.274181             933.120000           2488.320000         
model.layers.1.self_attn.o_proj               torch.Size([1, 6, 2048])       0.190944             0.310898             933.120000           2643.840000         
model.layers.1.post_attention_layernorm       torch.Size([1, 6, 2048])       0.424768             0.551224             933.120000           2487.296000         
model.layers.1.mlp.gate_proj                  torch.Size([1, 6, 8192])       0.420736             0.611782             1089.088000          3110.400000         
model.layers.1.mlp.act_fn                     torch.Size([1, 6, 8192])       0.136704             0.280619             933.120000           2332.800000         
model.layers.1.mlp.up_proj                    torch.Size([1, 6, 8192])       0.416288             0.566244             933.120000           2488.320000         
model.layers.1.mlp.down_proj                  torch.Size([1, 6, 2048])       0.432160             0.583649             1088.640000          3110.400000         
model.layers.2.input_layernorm                torch.Size([1, 6, 2048])       0.435840             0.548840             933.120000           2642.752000         
model.layers.2.self_attn.q_proj               torch.Size([1, 6, 2048])       0.223296             0.353813             933.504000           2332.800000         
model.layers.2.self_attn.k_proj               torch.Size([1, 6, 512])        0.190528             0.318527             933.120000           2332.800000         
model.layers.2.self_attn.v_proj               torch.Size([1, 6, 512])        0.161440             0.266075             933.120000           2642.752000         
model.layers.2.self_attn.o_proj               torch.Size([1, 6, 2048])       0.185088             0.316620             933.504000           2488.320000         
model.layers.2.post_attention_layernorm       torch.Size([1, 6, 2048])       0.388224             0.508547             933.504000           2177.280000         
model.layers.2.mlp.gate_proj                  torch.Size([1, 6, 8192])       0.419392             0.576973             1088.640000          2332.800000         
model.layers.2.mlp.act_fn                     torch.Size([1, 6, 8192])       0.116480             0.229836             933.504000           2799.360000         
model.layers.2.mlp.up_proj                    torch.Size([1, 6, 8192])       0.423744             0.591993             1088.640000          2642.752000         
model.layers.2.mlp.down_proj                  torch.Size([1, 6, 2048])       0.441888             0.628710             1088.640000          2799.360000         
model.layers.3.input_layernorm                torch.Size([1, 6, 2048])       0.391360             0.501871             933.504000           2642.752000         
model.layers.3.self_attn.q_proj               torch.Size([1, 6, 2048])       0.189376             0.284433             933.120000           2488.320000         
model.layers.3.self_attn.k_proj               torch.Size([1, 6, 512])        0.161120             0.294209             933.504000           2799.360000         
model.layers.3.self_attn.v_proj               torch.Size([1, 6, 512])        0.182688             0.285625             933.504000           3109.120000         
model.layers.3.self_attn.o_proj               torch.Size([1, 6, 2048])       0.213920             0.366688             933.120000           2488.320000         
model.layers.3.post_attention_layernorm       torch.Size([1, 6, 2048])       0.434752             0.548601             933.504000           2488.320000         
model.layers.3.mlp.gate_proj                  torch.Size([1, 6, 8192])       0.415392             0.550985             1088.640000          2643.840000         
model.layers.3.mlp.act_fn                     torch.Size([1, 6, 8192])       0.107808             0.214338             933.504000           2954.880000         
model.layers.3.mlp.up_proj                    torch.Size([1, 6, 8192])       0.414784             0.540495             1089.088000          2332.800000         
model.layers.3.mlp.down_proj                  torch.Size([1, 6, 2048])       0.469280             0.663042             1088.640000          2332.800000         
model.layers.4.input_layernorm                torch.Size([1, 6, 2048])       0.453696             0.567675             933.504000           2643.840000         
model.layers.4.self_attn.q_proj               torch.Size([1, 6, 2048])       0.193376             0.295162             933.120000           2487.296000         
model.layers.4.self_attn.k_proj               torch.Size([1, 6, 512])        0.164800             0.287056             933.504000           3110.400000         
model.layers.4.self_attn.v_proj               torch.Size([1, 6, 512])        0.161472             0.266314             932.736000           2488.320000         
model.layers.4.self_attn.o_proj               torch.Size([1, 6, 2048])       0.192320             0.312090             933.120000           2487.296000         
model.layers.4.post_attention_layernorm       torch.Size([1, 6, 2048])       0.449920             0.571012             933.504000           2954.880000         
model.layers.4.mlp.gate_proj                  torch.Size([1, 6, 8192])       0.428480             0.621796             1089.088000          2177.280000         
model.layers.4.mlp.act_fn                     torch.Size([1, 6, 8192])       0.112416             0.221491             932.736000           2488.320000         
model.layers.4.mlp.up_proj                    torch.Size([1, 6, 8192])       0.412640             0.564814             1089.088000          2954.880000         
model.layers.4.mlp.down_proj                  torch.Size([1, 6, 2048])       0.444992             0.605822             1089.088000          2642.752000         
model.layers.5.input_layernorm                torch.Size([1, 6, 2048])       0.443872             0.561476             933.120000           2643.840000         
model.layers.5.self_attn.q_proj               torch.Size([1, 6, 2048])       0.220064             0.317574             933.504000           2488.320000         
model.layers.5.self_attn.k_proj               torch.Size([1, 6, 512])        0.170304             0.303984             933.120000           2799.360000         
model.layers.5.self_attn.v_proj               torch.Size([1, 6, 512])        0.162144             0.282288             933.120000           2643.840000         
model.layers.5.self_attn.o_proj               torch.Size([1, 6, 2048])       0.187104             0.321150             933.504000           2799.360000         
model.layers.5.post_attention_layernorm       torch.Size([1, 6, 2048])       0.392544             0.506401             933.504000           2487.296000         
model.layers.5.mlp.gate_proj                  torch.Size([1, 6, 8192])       0.435008             0.630379             1088.640000          2488.320000         
model.layers.5.mlp.act_fn                     torch.Size([1, 6, 8192])       0.116128             0.239611             933.120000           2643.840000         
model.layers.5.mlp.up_proj                    torch.Size([1, 6, 8192])       0.418560             0.590324             1088.640000          2643.840000         
model.layers.5.mlp.down_proj                  torch.Size([1, 6, 2048])       0.451168             0.606537             1089.088000          2487.296000         
model.layers.6.input_layernorm                torch.Size([1, 6, 2048])       0.445600             0.565290             933.120000           2488.320000         
model.layers.6.self_attn.q_proj               torch.Size([1, 6, 2048])       0.198624             0.300884             933.504000           3110.400000         
model.layers.6.self_attn.k_proj               torch.Size([1, 6, 512])        0.249280             0.385523             933.504000           2799.360000         
model.layers.6.self_attn.v_proj               torch.Size([1, 6, 512])        0.189664             0.298500             933.504000           2642.752000         
model.layers.6.self_attn.o_proj               torch.Size([1, 6, 2048])       0.201216             0.359774             932.736000           2643.840000         
model.layers.6.post_attention_layernorm       torch.Size([1, 6, 2048])       0.444896             0.591993             933.504000           2954.880000         
model.layers.6.mlp.gate_proj                  torch.Size([1, 6, 8192])       0.425568             0.640869             1089.088000          2799.360000         
model.layers.6.mlp.act_fn                     torch.Size([1, 6, 8192])       0.119168             0.234604             933.504000           2487.296000         
model.layers.6.mlp.up_proj                    torch.Size([1, 6, 8192])       0.414624             0.545502             1088.640000          2488.320000         
model.layers.6.mlp.down_proj                  torch.Size([1, 6, 2048])       0.466080             0.599861             1089.088000          2954.880000         
model.layers.7.input_layernorm                torch.Size([1, 6, 2048])       0.433184             0.571966             933.120000           2487.296000         
model.layers.7.self_attn.q_proj               torch.Size([1, 6, 2048])       0.187872             0.292063             933.504000           2799.360000         
model.layers.7.self_attn.k_proj               torch.Size([1, 6, 512])        0.167104             0.271320             933.120000           2488.320000         
model.layers.7.self_attn.v_proj               torch.Size([1, 6, 512])        0.158624             0.262499             933.120000           2643.840000         
model.layers.7.self_attn.o_proj               torch.Size([1, 6, 2048])       0.187360             0.329256             933.120000           2643.840000         
model.layers.7.post_attention_layernorm       torch.Size([1, 6, 2048])       0.429856             0.544071             933.504000           2799.360000         
model.layers.7.mlp.gate_proj                  torch.Size([1, 6, 8192])       0.422944             0.602722             1089.088000          2487.296000         
model.layers.7.mlp.act_fn                     torch.Size([1, 6, 8192])       0.111904             0.226498             932.736000           2799.360000         
model.layers.7.mlp.up_proj                    torch.Size([1, 6, 8192])       0.413984             0.554562             1088.640000          3110.400000         
model.layers.7.mlp.down_proj                  torch.Size([1, 6, 2048])       0.432896             0.586987             1089.088000          2954.880000         
model.layers.8.input_layernorm                torch.Size([1, 6, 2048])       0.467040             0.580311             933.504000           2331.840000         
model.layers.8.self_attn.q_proj               torch.Size([1, 6, 2048])       0.185088             0.287771             933.504000           2954.880000         
model.layers.8.self_attn.k_proj               torch.Size([1, 6, 512])        0.157472             0.265598             933.120000           2487.296000         
model.layers.8.self_attn.v_proj               torch.Size([1, 6, 512])        0.182400             0.287294             933.504000           2643.840000         
model.layers.8.self_attn.o_proj               torch.Size([1, 6, 2048])       0.188320             0.310659             933.504000           2177.280000         
model.layers.8.post_attention_layernorm       torch.Size([1, 6, 2048])       0.366880             0.500917             933.120000           2488.320000         
model.layers.8.mlp.gate_proj                  torch.Size([1, 6, 8192])       0.457632             0.614166             1088.640000          2488.320000         
model.layers.8.mlp.act_fn                     torch.Size([1, 6, 8192])       0.135520             0.247717             933.120000           2487.296000         
model.layers.8.mlp.up_proj                    torch.Size([1, 6, 8192])       0.434560             0.575781             1088.640000          2488.320000         
model.layers.8.mlp.down_proj                  torch.Size([1, 6, 2048])       0.428608             0.564337             1089.088000          2642.752000         
model.layers.9.input_layernorm                torch.Size([1, 6, 2048])       0.392576             0.503540             933.120000           2799.360000         
model.layers.9.self_attn.q_proj               torch.Size([1, 6, 2048])       0.187840             0.300646             933.504000           2488.320000         
model.layers.9.self_attn.k_proj               torch.Size([1, 6, 512])        0.183328             0.302792             933.504000           2488.320000         
model.layers.9.self_attn.v_proj               torch.Size([1, 6, 512])        0.156832             0.263214             933.120000           2954.880000         
model.layers.9.self_attn.o_proj               torch.Size([1, 6, 2048])       0.208224             0.323772             933.504000           2954.880000         
model.layers.9.post_attention_layernorm       torch.Size([1, 6, 2048])       0.414208             0.526428             933.504000           2487.296000         
model.layers.9.mlp.gate_proj                  torch.Size([1, 6, 8192])       0.423424             0.574827             1088.640000          1710.720000         
model.layers.9.mlp.act_fn                     torch.Size([1, 6, 8192])       0.107808             0.215054             933.504000           3110.400000         
model.layers.9.mlp.up_proj                    torch.Size([1, 6, 8192])       0.434304             0.587702             1089.088000          2488.320000         
model.layers.9.mlp.down_proj                  torch.Size([1, 6, 2048])       0.452192             0.595808             933.120000           2643.840000         
model.layers.10.input_layernorm               torch.Size([1, 6, 2048])       0.384000             0.496149             933.120000           2487.296000         
model.layers.10.self_attn.q_proj              torch.Size([1, 6, 2048])       0.186304             0.298977             933.504000           2954.880000         
model.layers.10.self_attn.k_proj              torch.Size([1, 6, 512])        0.159808             0.312328             933.120000           2331.840000         
model.layers.10.self_attn.v_proj              torch.Size([1, 6, 512])        0.155552             0.261068             933.120000           2488.320000         
model.layers.10.self_attn.o_proj              torch.Size([1, 6, 2048])       0.181312             0.296831             933.504000           3110.400000         
model.layers.10.post_attention_layernorm      torch.Size([1, 6, 2048])       0.406464             0.517368             933.504000           2487.296000         
model.layers.10.mlp.gate_proj                 torch.Size([1, 6, 8192])       0.413088             0.566483             1089.088000          2488.320000         
model.layers.10.mlp.act_fn                    torch.Size([1, 6, 8192])       0.172096             0.281811             933.120000           2488.320000         
model.layers.10.mlp.up_proj                   torch.Size([1, 6, 8192])       0.410720             0.551701             1089.088000          2799.360000         
model.layers.10.mlp.down_proj                 torch.Size([1, 6, 2048])       0.426144             0.561714             1088.640000          2488.320000         
model.layers.11.input_layernorm               torch.Size([1, 6, 2048])       0.453472             0.563860             933.120000           2331.840000         
model.layers.11.self_attn.q_proj              torch.Size([1, 6, 2048])       0.183808             0.275135             933.120000           2488.320000         
model.layers.11.self_attn.k_proj              torch.Size([1, 6, 512])        0.162112             0.266552             933.504000           2798.208000         
model.layers.11.self_attn.v_proj              torch.Size([1, 6, 512])        0.180512             0.284195             933.120000           3110.400000         
model.layers.11.self_attn.o_proj              torch.Size([1, 6, 2048])       0.185600             0.303507             933.504000           2642.752000         
model.layers.11.post_attention_layernorm      torch.Size([1, 6, 2048])       0.362848             0.476599             933.120000           2332.800000         
model.layers.11.mlp.gate_proj                 torch.Size([1, 6, 8192])       0.453088             0.608683             1089.088000          2799.360000         
model.layers.11.mlp.act_fn                    torch.Size([1, 6, 8192])       0.142912             0.251293             933.504000           2799.360000         
model.layers.11.mlp.up_proj                   torch.Size([1, 6, 8192])       0.444160             0.588179             1089.088000          2487.296000         
model.layers.11.mlp.down_proj                 torch.Size([1, 6, 2048])       0.424800             0.611305             1088.640000          2488.320000         
model.layers.12.input_layernorm               torch.Size([1, 6, 2048])       0.406752             0.516415             932.736000           2487.296000         
model.layers.12.self_attn.q_proj              torch.Size([1, 6, 2048])       0.189440             0.283480             933.120000           2954.880000         
model.layers.12.self_attn.k_proj              torch.Size([1, 6, 512])        0.205216             0.309706             933.120000           2488.320000         
model.layers.12.self_attn.v_proj              torch.Size([1, 6, 512])        0.157408             0.284910             933.504000           2954.880000         
model.layers.12.self_attn.o_proj              torch.Size([1, 6, 2048])       0.212992             0.338316             933.120000           2642.752000         
model.layers.12.post_attention_layernorm      torch.Size([1, 6, 2048])       0.394112             0.508547             933.120000           2488.320000         
model.layers.12.mlp.gate_proj                 torch.Size([1, 6, 8192])       0.414784             0.563860             1088.640000          2488.320000         
model.layers.12.mlp.act_fn                    torch.Size([1, 6, 8192])       0.108128             0.216484             933.120000           4506.368000         
model.layers.12.mlp.up_proj                   torch.Size([1, 6, 8192])       0.455872             0.682354             1089.088000          2332.800000         
model.layers.12.mlp.down_proj                 torch.Size([1, 6, 2048])       0.428736             0.565767             1088.640000          2487.296000         
model.layers.13.input_layernorm               torch.Size([1, 6, 2048])       0.484128             0.602484             933.120000           2488.320000         
model.layers.13.self_attn.q_proj              torch.Size([1, 6, 2048])       0.214464             0.326157             933.120000           2643.840000         
model.layers.13.self_attn.k_proj              torch.Size([1, 6, 512])        0.176352             0.285625             933.120000           3109.120000         
model.layers.13.self_attn.v_proj              torch.Size([1, 6, 512])        0.197536             0.305653             933.120000           6065.280000         
model.layers.13.self_attn.o_proj              torch.Size([1, 6, 2048])       0.205184             0.340939             933.120000           2799.360000         
model.layers.13.post_attention_layernorm      torch.Size([1, 6, 2048])       0.513728             0.653505             933.120000           3265.920000         
model.layers.13.mlp.gate_proj                 torch.Size([1, 6, 8192])       0.425056             0.579357             1089.088000          2643.840000         
model.layers.13.mlp.act_fn                    torch.Size([1, 6, 8192])       0.110592             0.217915             933.120000           2488.320000         
model.layers.13.mlp.up_proj                   torch.Size([1, 6, 8192])       0.416480             0.561953             1087.296000          2951.232000         
model.layers.13.mlp.down_proj                 torch.Size([1, 6, 2048])       0.438176             0.581503             932.736000           3264.576000         
model.layers.14.input_layernorm               torch.Size([1, 6, 2048])       0.421056             0.559330             933.120000           3261.888000         
model.layers.14.self_attn.q_proj              torch.Size([1, 6, 2048])       0.248096             0.340462             932.352000           3888.000000         
model.layers.14.self_attn.k_proj              torch.Size([1, 6, 512])        0.175424             0.283241             933.120000           2488.320000         
model.layers.14.self_attn.v_proj              torch.Size([1, 6, 512])        0.174432             0.297546             933.120000           2488.320000         
model.layers.14.self_attn.o_proj              torch.Size([1, 6, 2048])       0.184544             0.319242             933.120000           2487.296000         
model.layers.14.post_attention_layernorm      torch.Size([1, 6, 2048])       0.404736             0.518322             933.504000           3109.120000         
model.layers.14.mlp.gate_proj                 torch.Size([1, 6, 8192])       0.457312             0.604153             1089.088000          2487.296000         
model.layers.14.mlp.act_fn                    torch.Size([1, 6, 8192])       0.127520             0.236273             933.504000           2488.320000         
model.layers.14.mlp.up_proj                   torch.Size([1, 6, 8192])       0.438304             0.588179             1088.640000          2954.880000         
model.layers.14.mlp.down_proj                 torch.Size([1, 6, 2048])       0.427168             0.571728             1089.088000          3110.400000         
model.layers.15.input_layernorm               torch.Size([1, 6, 2048])       0.404512             0.516653             933.120000           2488.320000         
model.layers.15.self_attn.q_proj              torch.Size([1, 6, 2048])       0.186816             0.277281             932.736000           2488.320000         
model.layers.15.self_attn.k_proj              torch.Size([1, 6, 512])        0.185472             0.319481             933.120000           2643.840000         
model.layers.15.self_attn.v_proj              torch.Size([1, 6, 512])        0.183616             0.290155             933.120000           2643.840000         
model.layers.15.self_attn.o_proj              torch.Size([1, 6, 2048])       0.192768             0.373840             932.736000           3884.800000         
model.layers.15.post_attention_layernorm      torch.Size([1, 6, 2048])       0.382240             0.499487             932.736000           5594.112000         
model.layers.15.mlp.gate_proj                 torch.Size([1, 6, 8192])       0.450496             0.621557             1089.088000          3110.400000         
model.layers.15.mlp.act_fn                    torch.Size([1, 6, 8192])       0.127456             0.237703             933.120000           2331.840000         
model.layers.15.mlp.up_proj                   torch.Size([1, 6, 8192])       0.417344             0.550747             1088.640000          2643.840000         
model.layers.15.mlp.down_proj                 torch.Size([1, 6, 2048])       0.432096             0.564337             1089.088000          3421.440000         
model.norm                                    torch.Size([1, 6, 2048])       0.406304             0.517607             933.120000           2487.296000         
lm_head                                       torch.Size([1, 6, 128256])     4.917344             5.083561             5287.680000          2487.296000         
